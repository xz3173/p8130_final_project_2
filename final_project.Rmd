---
title: "Final Project"
output: github_document
---

```{r}
library(gtsummary)
library(tidyverse)
library(car)
library(caret)
library(corrplot)
library(glmnet)
library(leaps)
library(pROC)
library(broom)

set.seed(123)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))

```

```{r}
data = read_csv("./data/data.csv") |>
  janitor::clean_names()

#summary(data)
data=data|>select(-survival_months)
#str(data)
```

```{r}
# Check for missing data
#colSums(is.na(data))
```

```{r}
# Convert character to factor for regression analysis
clean_data = data |>
  mutate(
    race = as.numeric(factor(race, levels = c("White", "Black", "Other"))) - 1,
    marital_status = as.numeric(factor(marital_status, levels = c("Married", "Single", "Divorced", "Widowed", "Separated"))) - 1,
    t_stage = as.numeric(factor(t_stage, levels = c("T1", "T2", "T3", "T4"))) - 1,
    n_stage = as.numeric(factor(n_stage, levels = c("N1", "N2", "N3"))) - 1,
    x6th_stage = as.numeric(factor(x6th_stage, levels = c("IIA", "IIB", "IIIA", "IIIB", "IIIC"))) - 1,
    differentiate = as.numeric(factor(differentiate, levels = c("Undifferentiated", "Poorly differentiated", "Moderately differentiated", "Well differentiated"))) - 1,
    grade = as.numeric(factor(grade, levels = c("1", "2", "3", "anaplastic; Grade IV"))),
    a_stage = as.numeric(factor(a_stage, levels = c("Regional", "Distant"))) - 1,
    estrogen_status = as.numeric(factor(estrogen_status, levels = c("Negative", "Positive"))) - 1,
    progesterone_status = as.numeric(factor(progesterone_status, levels = c("Negative", "Positive"))) - 1,
    status = as.numeric(factor(status, levels = c("Dead", "Alive"))) - 1)|>
  rename(regional_node_positive = reginol_node_positive)


```



#Model fitting
#Based on boxplots, transformaiton is necesessary to reduce outliers 
#cube root of tumor size
#log of regional_node_examied
#log of regional_node_positive
#Figure 1 
```{r}
proj2 = data |>
tbl_summary(by="status",
  missing_text = "(Missing)", # counts missing values
  statistic = list(all_continuous() ~ "mean={mean} (min={min}, max={max}, sd={sd})",
                   all_categorical() ~ "n={n} (p={p}%)") # stats for categorical
  ) |>
bold_labels()  |>
italicize_levels()


clean_data2=clean_data
clean_data2$tumor_size= (clean_data$tumor_size)^(1/3)
clean_data2$regional_node_examined = log(clean_data$regional_node_examined)
clean_data2$regional_node_positive = log(clean_data$regional_node_positive)

```

# plotting histogram
```{r}
plot_histogram = 
  function(data_vector, main_title, x_label = "") {
  hist(data_vector, 
       main = main_title, 
       xlab = x_label, 
       col = "blue")
}


png("plots/histogram_plot.png", 
    width = 12 * 600, 
    height = 12 * 600, res = 600)

par(mar = c(2, 2, 2, 2))
par(mfrow = c(4, 4))


column_names = names(clean_data)
for (col_name in column_names) {
  plot_histogram(clean_data[[col_name]], 
                 main_title = col_name, 
                 x_label = col_name)
}

dev.off()
```

#Find correlation
```{R}
corplot=cor(clean_data2)
corrplot(corplot)
#tumor_size vs t_stage = 0.801
#grade=differentiate =>1
#n_stage = x6th_stage => 0.881
#n_stage = regional positive status =>0.838073333
selected_data = clean_data2 |>
  select(-tumor_size, -grade,-n_stage,-regional_node_positive,-x6th_stage)

corplot=cor(selected_data)
corrplot(corplot)
```

#Separate training and testing set (80% training 20% testing )
```{R}
# Calculate the size of each of the data sets
data_size <- nrow(clean_data2)
train_size <- floor(0.8 * data_size)

# Create a random sample of row indices for the training set
train_indices <- sample(sample(seq_len(data_size), size = train_size))

# Subset the data into training and testing sets
train_set <- clean_data2[train_indices, ]
selectedData_train_set<- selected_data[train_indices,]
test_set <- clean_data2[-train_indices, ]
selectedData_test_set <- selected_data[-train_indices, ]

```

# Fit a full model
```{R}

selected_train = train_set |>
  select(-tumor_size, -grade,-n_stage,-regional_node_positive,-x6th_stage)

null_model = glm(status ~ 1, family = binomial(link = "logit"), data = selected_train)

full_model=glm(status ~ . , family = binomial(link = "logit"), data = selected_train)
```

# Check logistic regression assumptions

Binary logistic regression relies on underlying assumptions to be true:

1.The outcome is a binary or dichotomous variable like yes vs no, positive vs negative, 1 vs 0.
2.There is a linear relationship between the logit of the outcome(status) and each predictor variables. Recall that the logit function is logit(p) = log(p/(1-p)), where p is the probabilities of the outcome.
3.There is no influential values in the continuous predictors.
4.There is no multicollinearity among the predictors.

## Checking Linearity of continuous variables to the response
##HENRY - MAY BE NOT NECESSARY
```{r}
# Check linearity assumption using conditional residual plots
crPlots(full_model)
```

## Checking for Influential Observations:
Cook's Distance:
```{r}
# Calculate Cook's distance for the full model
cooksd <- influence.measures(full_model)$cooks

# Identify influential observations
influential_points <- which(cooksd > 4 / length(cooksd))

# Display influential points
print(influential_points)
```

## Checking for Multicollinearity:
```{r}
# Check for multicollinearity using VIF
vif_values <- car::vif(full_model)

# Display VIF values
print(vif_values)
```
As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. In our example, there is no collinearity: all variables have a value of VIF well below 5.

# Using Forward, BackWard
```{r}
step_modelF = step(null_model, scope = list(lower = null_model, upper = full_model), 
                   direction = "forward")
step_model = step(full_model, direction = "backward")
summary(step_model)
summary(step_modelF)
anova(step_model,step_modelF,test="Chisq")
test_predictions_log_oddsStep <- predict(step_model, newdata  = (test_set),type='response')
test_predictions_probStep <- plogis(test_predictions_log_oddsStep)
roc_curveStep <- roc(response = (test_set$status), predictor = as.numeric(test_predictions_probStep))
auc(roc_curveStep)


```

```{r}
#if (!dir.exists("plots")) {
#    dir.create("plots")
#}
#
#png("plots/pairs_plot.png", 
#    width = 12 * 600, 
#    height = 12 * 600, 
#    res = 600)

#pairs(clean_data)
```

# Corr plot
```{r}
#png("plots/corr_plot.png", 
#    width = 12 * 600, 
#    height = 12 * 600, 
#    res = 600)

#corrplot(cor(clean_data), type = "upper", diag = FALSE)

```

#Elastic Net
```{R}

# Prepare your data
X <- as.matrix(train_set[, setdiff(names(train_set), "status")])  # Predictor variables
y <- train_set$status  # Response variable

lambda_seq <- 10^seq(-3, 0, by = .001)

# Use cross-validation to find the optimal lambda
cv_object <- cv.glmnet(X, y, family = "binomial", alpha = 0.5, type.measure = "class",nfolds=5, lambda = lambda_seq)

tibble(lambda = cv_object$lambda,
mean_cv_error = cv_object$cvm) %>%
ggplot(aes(x = lambda, y = mean_cv_error)) +
geom_point()

# Best lambda value
best_lambda <- cv_object$lambda.min
# Refit the model using the best lambda
final_model <- glmnet(X, y, family = "binomial", alpha = 0.5, lambda = best_lambda)

test_set2 <- test_set|> select(-status)
test_predictions_log_odds <- predict(final_model, newx = as.matrix(test_set2))

# Convert log-odds to probabilities
test_predictions_probElastic <- plogis(test_predictions_log_odds)
# Create the ROC curve
roc_curve <- roc(response = as.matrix(test_set$status), predictor = as.numeric(test_predictions_probElastic) )

auc(roc_curve)

```


#Elastic net 2
##Training without full variables
```{r}
X2 <- as.matrix(selectedData_train_set[, setdiff(names(selectedData_train_set), "status")])  # Predictor variables
# Use cross-validation to find the optimal lambda
cv_object <- cv.glmnet(X2, y, family = "binomial", alpha = 0.5, type.measure = "class",nfolds=5, lambda = lambda_seq)

tibble(lambda = cv_object$lambda,
mean_cv_error = cv_object$cvm) %>%
ggplot(aes(x = lambda, y = mean_cv_error)) +
geom_point()
# Best lambda value
best_lambda <- cv_object$lambda.min
# Refit the model using the best lambda
final_model2 <- glmnet(X2, y, family = "binomial", alpha = 0.5, lambda = best_lambda)

selectedData_test_set <- selectedData_test_set|> select(-status)
test_predictions_log_odds2 <- predict(final_model, newx = as.matrix(test_set2))

# Convert log-odds to probabilities
test_predictions_probElastic2 <- plogis(test_predictions_log_odds2)
# Create the ROC curve
roc_curvenet2 <- roc(response = (test_set$status), predictor = as.numeric(test_predictions_probElastic2) )

auc(roc_curvenet2)

plot(roc_curve, main = "ROC Curve", col = "#1c61b6", lwd = 2)
lines(roc_curveStep,col='yellow')
lines(roc_curvenet2,col='green')
```



# plotting histogram
## final model diagnostics
1. Coefficient Path Plot:
```{r}
plot(final_model, xvar = "lambda")
```
2.Cross-Validation Plot:
```{r}
plot(cv_object)
```
3.Deviance Plot:
```{r}
plot(final_model, xvar = "lambda", label = TRUE)
```
4.Predicted vs. Observed Plot:
```{R}
par(mfrow = c(1, 2))
plot(predict(final_model, s = best_lambda, newx = as.matrix(test_set2)), as.numeric(test_set$status), main = "Predicted vs. Observed")
abline(a = 0, b = 1, col = "red")
```
5.Residual Analysis:
```{r}
residuals <- as.vector(predict(final_model, newx = as.matrix(test_set2), s = best_lambda, type = "response") - as.numeric(test_set$status))
plot(residuals, main = "Residuals vs. Fitted Values", xlab = "Fitted Values", ylab = "Residuals")
```


# Assess final model assumptions
Checking for violations of regression model assumptions, influential observations, and multicollinearity is an essential part of ensuring the reliability and validity of our logistic regression model. 

1. Checking Linearity Assumption:
While glmnet is based on regularization and not least squares, the linearity assumption can be assessed by examining the relationship between predicted and observed values.

```{r}
# Assuming 'final_model' is your glmnet logistic regression model
# Plot predicted vs. observed values
plot(predict(final_model, newx = as.matrix(test_set2), s = best_lambda, type = "response"), as.numeric(test_set$status), main = "Predicted vs. Observed")
abline(a = 0, b = 1, col = "red")
```

2. Checking Residuals:
#NOT NEEDED, LOGISTICS regression does not need this plot, becasue it's logistics is none-linear
Although there are no standard residuals, we can examine the differences between predicted and observed values.
```{r}
# Assuming 'final_model' is your glmnet logistic regression model
residuals <- as.vector(predict(final_model2, newx = as.matrix(selectedData_test_set), s = best_lambda, type = "response") - as.numeric(test_set$status))
plot(residuals, main = "Residuals vs. Fitted Values",  xlab = "Fitted Values", ylab = "Residuals")
```

3. Checking for Multicollinearity:
Evaluate multicollinearity using VIF (Variance Inflation Factor) to assess the relationships between predictors.
```{r}
# Extract coefficients from the final_model
coefficients <- as.matrix(coef(final_model2))

# Extract predictors and response
predictors <- coefficients[-1, ]
response <- coefficients[1, ]

# Combine predictors and response into a data frame
model_data <- data.frame(response = response, predictors)

# Calculate correlation matrix
cor_matrix <- cor(predictors, predictors)
```


# !! (Below part is the old version part, I'm not sure if this part should be removed. But you can refence this part!)

### Building a logistic regression model 

We start by computing an example of logistic regression model using the selected_data, for predicting the probability of status test positivity based on clinical variables.
```{r}
# Fit the logistic regression model
model2 <-  glm(status ~., data = selected_data, 
               family = binomial)
# Predict the probability (p) of diabete positivity
probabilities <- predict(model2, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "Alive", "Dead")
head(predicted.classes)
```

# Check logistic regression assumptions

Binary logistic regression relies on underlying assumptions to be true:

1.The outcome is a binary or dichotomous variable like yes vs no, positive vs negative, 1 vs 0.
#HENRY, ALREADY MET
2.There is a linear relationship between the logit of the outcome and each predictor variables. Recall that the logit function is logit(p) = log(p/(1-p)), where p is the probabilities of the outcome.
#HENRY- ALREADY MET BUY FITTING DATA
3.There is no influential values (extreme values or outliers) in the continuous predictors
# COOKS distant, however, you did , this is useful
4.There is no high intercorrelations (i.e. multicollinearity) among the predictors.
#HENRY- ALDAY DID FOR Selected_data, fitted 3 model.
### Linearity assumption (Not sure with this part)
Here, we’ll check the linear relationship between continuous predictor variables and the logit of the outcome. This can be done by visually inspecting the scatter plot between each predictor and the logit values.
```{r}
# Refit the model using the best lambda
final_model <- glmnet(X, y, family = "binomial", alpha = 0.5, lambda = best_lambda)

test_set2 <- test_set|> select(-status)
test_predictions_log_odds <- predict(final_model, newx = as.matrix(test_set2))
# Convert log-odds to probabilities
test_predictions_probElastic <- plogis(test_predictions_log_odds)
```

1. Remove qualitative variables from the original data frame and bind the logit values to the data:
#HENRY - NOT SURE what is this for
```{r}
# Select only numeric predictors
mydata <- selected_data %>%
  dplyr::select_if(is.numeric) 
predictors <- colnames(mydata)
# Bind the logit and tidying the data for plot
mydata <- mydata %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)
```

2. Create the scatter plots:
#HENRY - not helpful, this does not telling anything
```{r}
ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
```
A graphical check of linearity can be performed using a best fit “loess” line. This is on the probability scale, so it is not going to be straight. But it should be monotonic - it should only ever go up or down.

### Influential values
Influential values are extreme individual data points that can alter the quality of the logistic regression model.
##HENRY - THIS IS GOOD 
The most extreme values in the data can be examined by visualizing the Cook’s distance values. Here we label the top 3 largest values:
```{r}
plot(model2, which = 4, id.n = 3)
```
Note that, not all outliers are influential observations. To check whether the data contains potential influential observations, the standardized residual error can be inspected. Data points with an absolute standardized residuals above 3 represent possible outliers and may deserve closer attention.
#GOOD
```{r}
# Extract model results
model.data <- augment(model2) %>% 
  mutate(index = 1:n()) 

# The data for the top 3 largest values, according to the Cook’s distance, can be displayed as follow:
model.data %>% top_n(3, .cooksd)

# Plot the standardized residuals:
ggplot(model.data, aes(index, .std.resid)) + 
  geom_point(aes(color = status), alpha = .5) +
  theme_bw()

# Filter potential influential data points with abs(.std.res) > 3:
model.data %>% 
  filter(abs(.std.resid) > 3)
```

### Multicollinearity
#NOT NEED, Aldary did that, but ZHANGXUE might need this
Multicollinearity corresponds to a situation where the data contain highly correlated predictor variables.

Multicollinearity is an important issue in regression analysis and should be fixed by removing the concerned variables. It can be assessed using the R function vif(), which computes the variance inflation factors:
```{r}
car::vif(model2)
```
As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. In our example, there is no collinearity: all variables have a value of VIF well below 5.
